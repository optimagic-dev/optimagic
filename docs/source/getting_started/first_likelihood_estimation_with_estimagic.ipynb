{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordered Logit Example\n",
    "\n",
    "This notebook shows how to do a simple maximum likelihood (ml) estimation with estimagic. As illustrating example we implement an ordered logit model from scratch. \n",
    "\n",
    "\n",
    "1. Load and process the data\n",
    "2. Set up a likelihood function\n",
    "3. Maximize the likelihood function\n",
    "4. Calculate standard errors, P-values and confidence intervals\n",
    "\n",
    "The user only needs to do step 1 and 2. The rest is done by `estimate_ml`. \n",
    "\n",
    "To be very clear: Estimagic is not a package to estimate logit models or other models that are implemented in Stata, statsmodels or anywhere else. Its purpose is to estimate parameters with custom likelihood or method of simulated moments functions. We just use an ordered logit model as an example of a very simple likelihood function. \n",
    "\n",
    "The example we will use to test our model is taken from the [Stata Documentation](https://stats.idre.ucla.edu/stata/dae/ordered-logistic-regression/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from patsy import dmatrices\n",
    "from scipy import stats\n",
    "\n",
    "from estimagic import estimate_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the user input\n",
    "\n",
    "We choose an R-style formula as a convenient way of specifying the ordered logit model and use `patsy` to construct matrices from the dataset. \n",
    "\n",
    "We will need four inputs:\n",
    "\n",
    "1. A DataFrame with start parameters for the optimization.\n",
    "2. An array with the dependent variable.\n",
    "3. A 2d array with explanatory variables.\n",
    "4. Constraints for the optimization that keep the cutoffs increasing.\n",
    "\n",
    "We construct all of those inputs using the `ordered_logit_processing` function. You could also do those steps in a simple script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordered_logit_processing(formula, data):\n",
    "    \"\"\"Process user input for an ordered logit model.\"\"\"\n",
    "    # extract data arrays\n",
    "    y, x = dmatrices(formula + \" - 1\", data, return_type=\"dataframe\")\n",
    "    y = y[y.columns[0]]\n",
    "\n",
    "    # extract dimensions\n",
    "    num_choices = len(y.unique())\n",
    "    beta_names = list(x.columns)\n",
    "    num_betas = len(beta_names)\n",
    "    num_cutoffs = num_choices - 1\n",
    "\n",
    "    # set-up index for params_df\n",
    "    names = beta_names + list(range(num_cutoffs))\n",
    "    categories = [\"beta\"] * num_betas + [\"cutoff\"] * num_cutoffs\n",
    "    index = pd.MultiIndex.from_tuples(zip(categories, names), names=[\"type\", \"name\"])\n",
    "\n",
    "    # make params_df\n",
    "    np.random.seed(5471)\n",
    "    start_params = pd.DataFrame(index=index)\n",
    "    start_params[\"value\"] = np.hstack(\n",
    "        [\n",
    "            np.random.uniform(low=-0.5, high=0.5, size=len(x.columns)),\n",
    "            np.arange(num_cutoffs) * 2,\n",
    "        ]\n",
    "    )\n",
    "    start_params[\"group\"] = start_params.index.get_level_values(\"type\")\n",
    "\n",
    "    # make constraints\n",
    "    constr = [{\"loc\": \"cutoff\", \"type\": \"increasing\"}]\n",
    "\n",
    "    # turn pandas objects into numpy arrays\n",
    "    y_arr = y.to_numpy().astype(int)\n",
    "    x_arr = x.to_numpy()\n",
    "\n",
    "    return start_params, y_arr, x_arr, constr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the `loglike` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordered_logit_loglike(params, y, x):\n",
    "    \"\"\"Likelihood function of an orderd logit model.\"\"\"\n",
    "    # parse the parameter vector into its quantities\n",
    "    beta = params.loc[\"beta\", \"value\"].to_numpy()\n",
    "    cutoffs = params.loc[\"cutoff\", \"value\"].to_numpy()\n",
    "\n",
    "    # calculate deterministic part of utilities\n",
    "    xb = x.dot(beta)\n",
    "\n",
    "    # evaluate likelihood\n",
    "    upper_cutoffs = np.hstack([cutoffs, np.inf])[y]\n",
    "    lower_cutoffs = np.hstack([-np.inf, cutoffs])[y]\n",
    "    upper_cdf = stats.logistic.cdf(upper_cutoffs - xb)\n",
    "    lower_cdf = stats.logistic.cdf(lower_cutoffs - xb)\n",
    "\n",
    "    contributions = np.log(upper_cdf - lower_cdf)\n",
    "\n",
    "    res = {\"contributions\": contributions, \"value\": contributions.sum()}\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few remarks are in order:\n",
    "\n",
    "1. There are numerically better ways to calculate the likelihood, we chose this implementation for brevity and readability. \n",
    "2. The loglike function takes params and other arguments. You are completely flexible in the number and names of the other arguments as long as the first argument is params. \n",
    "3. The loglike function returns a dictionary with the entries \"contributions\" and \"value\". The \"contributions\" are the log likelihood evaluations of each individual in the dataset. The \"value\" are their sum. The \"value\" entry could be omitted, the \"contributions\" entry is mandatory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"ologit.pickle\")\n",
    "formula = \"apply ~ pared + public + gpa\"\n",
    "start_params, y, x, constraints = ordered_logit_processing(formula, data)\n",
    "\n",
    "res = estimate_ml(\n",
    "    loglike=ordered_logit_loglike,\n",
    "    params=start_params,\n",
    "    optimize_options={\"algorithm\": \"scipy_lbfgsb\"},\n",
    "    constraints=constraints,\n",
    "    loglike_kwargs={\"y\": y, \"x\": x},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>standard_error</th>\n",
       "      <th>p_value</th>\n",
       "      <th>ci_lower</th>\n",
       "      <th>ci_upper</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">beta</th>\n",
       "      <th>pared</th>\n",
       "      <td>1.048</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.507</td>\n",
       "      <td>1.589</td>\n",
       "      <td>***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>public</th>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.811</td>\n",
       "      <td>-0.587</td>\n",
       "      <td>0.469</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpa</th>\n",
       "      <td>0.616</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.077</td>\n",
       "      <td>1.155</td>\n",
       "      <td>**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">cutoff</th>\n",
       "      <th>0</th>\n",
       "      <td>2.203</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.592</td>\n",
       "      <td>3.815</td>\n",
       "      <td>***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.299</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.641</td>\n",
       "      <td>5.956</td>\n",
       "      <td>***</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               value  standard_error  p_value  ci_lower  ci_upper stars\n",
       "type   name                                                            \n",
       "beta   pared   1.048           0.276    0.000     0.507     1.589   ***\n",
       "       public -0.059           0.269    0.811    -0.587     0.469      \n",
       "       gpa     0.616           0.275    0.025     0.077     1.155    **\n",
       "cutoff 0       2.203           0.822    0.007     0.592     3.815   ***\n",
       "       1       4.299           0.846    0.000     2.641     5.956   ***"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"summary_jacobian\"].round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's in the results?\n",
    "\n",
    "The result of `estimate_ml` is a dictionary with the following entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['summary_jacobian', 'cov_jacobian', 'jacobian', 'hessian', 'optimize_res', 'jacobian_numdiff_info'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, we might have several summaries and several cov entries. This is because we always all possible types of standard errors, so you can compare them easily (for example using our estimation table functions). \n",
    "\n",
    "In the current example we only have jacobian based standard errors because we did not provide a closed form hessian function and numerical hessians are not yet implemented. With a closed form hessian we would also get hessian based and robust standard errors. Even cluster and strata robust standard errors are possible if you provide the relevant information. \n",
    "\n",
    "If numerical optimizations or derivative calculations were performed, the full output of those steps is also part of the results dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to STATA's results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>stata_value</th>\n",
       "      <th>stata_standard_error</th>\n",
       "      <th>stata_p_value</th>\n",
       "      <th>stata_ci_lower</th>\n",
       "      <th>stata_ci_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pared</td>\n",
       "      <td>1.048</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.527</td>\n",
       "      <td>1.569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>public</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.844</td>\n",
       "      <td>-0.642</td>\n",
       "      <td>0.525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpa</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.105</td>\n",
       "      <td>1.127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cut1</td>\n",
       "      <td>2.203</td>\n",
       "      <td>0.780</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.675</td>\n",
       "      <td>3.731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cut2</td>\n",
       "      <td>4.299</td>\n",
       "      <td>0.804</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.722</td>\n",
       "      <td>5.875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name  stata_value  stata_standard_error  stata_p_value  stata_ci_lower  \\\n",
       "0   pared        1.048                 0.266          0.000           0.527   \n",
       "1  public       -0.059                 0.298          0.844          -0.642   \n",
       "2     gpa        0.616                 0.261          0.018           0.105   \n",
       "3    cut1        2.203                 0.780            NaN           0.675   \n",
       "4    cut2        4.299                 0.804            NaN           2.722   \n",
       "\n",
       "   stata_ci_upper  \n",
       "0           1.569  \n",
       "1           0.525  \n",
       "2           1.127  \n",
       "3           3.731  \n",
       "4           5.875  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stata_results = pd.read_csv(\"stata_ologit_results.csv\")\n",
    "stata_results.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty good! The parameter estimates line up perfectly. The standard errors are slightly off. This comes from the following differences between our implementation and the stata one:\n",
    "- We currently only support jacobian based standard errors in combination with constraints and use a parametric bootstrap to enforce the constraints. This introduces a sampling error.\n",
    "- We used numerical derivatives to keep the example simple. Stata implements a closed form derivative. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
