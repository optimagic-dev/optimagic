{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(how-to-select-algorithms)=\n",
    "# How to select a local optimizer\n",
    "\n",
    "This guide explains how to choose a local optimizer that works well for your problem. \n",
    "Depending on your [strategy for global optimization](how_to_globalization.ipynb) it \n",
    "is also relevant for global optimization problems. \n",
    "\n",
    "## Important facts \n",
    "\n",
    "- There is no optimizer that works well for all problems \n",
    "- Making the right choice can lead to enormous speedups\n",
    "- Making the wrong choice can mean that you cannot solve your problem at all\n",
    "\n",
    "\n",
    "## The four steps for selecting algorithms\n",
    "\n",
    "Algorithm selection is a mix of theory and experimentation. We recommend the following \n",
    "for steps:\n",
    "\n",
    "1. Theory: Select three to 5 candidate algorithms based on the properties \n",
    "of your problem. Below we provide a simple decision tree for this step.\n",
    "2. Experiments: Run the candidate algorithms fo a small number of function \n",
    "evaluations. As a rule of thumb, use between `n_params` and `10 * n_params`\n",
    "evaluations. \n",
    "3. Comparison: Compare the results in a criterion plot.\n",
    "4. Optimization: Re-run the optimization algorithm with the best results until \n",
    "convergence. Use the best parameter vector from the experiments as starting point.\n",
    "\n",
    "These steps work well for most problems. Sometimes you need [variations](four-steps-variations).\n",
    "\n",
    "## An example problem\n",
    "\n",
    "As an example we use the Trid function. The Trid function has no local minimum except \n",
    "the global one. It is defined for any number of dimensions, we will pick 20. As starting \n",
    "values we will pick the vector [0, 1, ..., 19]. \n",
    "\n",
    "A Python implementation of the function and its gradient looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import optimagic as om\n",
    "\n",
    "\n",
    "def trid_scalar(x):\n",
    "    \"\"\"Implement Trid function: https://www.sfu.ca/~ssurjano/trid.html.\"\"\"\n",
    "    return ((x - 1) ** 2).sum() - (x[1:] * x[:-1]).sum()\n",
    "\n",
    "\n",
    "def trid_gradient(x):\n",
    "    \"\"\"Calculate gradient of trid function.\"\"\"\n",
    "    l1 = np.insert(x, 0, 0)\n",
    "    l1 = np.delete(l1, [-1])\n",
    "    l2 = np.append(x, 0)\n",
    "    l2 = np.delete(l2, [0])\n",
    "    return 2 * (x - 1) - l1 - l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Theory\n",
    "\n",
    "The below decision tree offers a practical guide on how to narrow down the set of algorithms to experiment with, based on the theoretical properties of your problem:\n",
    "\n",
    "```{mermaid}\n",
    "graph LR\n",
    "    classDef highlight fill:#FF4500;\n",
    "    A[\"Do you have<br/>nonlinear constraints?\"] -- yes --> B[\"differentiable?\"]\n",
    "    B[\"differentiable?\"] -- yes --> C[\"'ipopt', 'nlopt_slsqp', 'scipy_trust_constr'\"]\n",
    "    B[\"differentiable?\"] -- no --> D[\"'scipy_cobyla', 'nlopt_cobyla'\"]\n",
    "\n",
    "    A[\"Do you have<br/>nonlinear constraints?\"] -- no --> E[\"Can you exploit<br/>a least-squares<br/>structure?\"]\n",
    "    E[\"Can you exploit<br/>a least-squares<br/>structure?\"] -- yes --> F[\"differentiable?\"]\n",
    "    E[\"Can you exploit<br/>a least-squares<br/>structure?\"] -- no --> G[\"differentiable?\"]\n",
    "\n",
    "    F[\"differentiable?\"] -- yes --> H[\"'scipy_ls_lm', 'scipy_ls_trf', 'scipy_ls_dogleg'\"]\n",
    "    F[\"differentiable?\"] -- no --> I[\"'nag_dflos', 'pounders', 'tao_pounders'\"]\n",
    "\n",
    "    G[\"differentiable?\"] -- yes --> J[\"'scipy_lbfgsb', 'nlopt_lbfgsb', 'fides'\"]\n",
    "    G[\"differentiable?\"] -- no --> K[\"'nlopt_bobyqa', 'nlopt_neldermead', 'neldermead_parallel'\"]\n",
    "```\n",
    "\n",
    "Let's go through the steps for the Trid function:\n",
    "\n",
    "1. There are no nonlinear constraints our solution needs to satisfy\n",
    "2. There is no least-squares structure we can exploit \n",
    "3. The function is differentiable and we have a closed form gradient that we would like \n",
    "to use. \n",
    "\n",
    "We therefore end up with the candidate algorithms `scipy_lbfgsb`, `nlopt_lbfgsb`, and \n",
    "`fides`.\n",
    "\n",
    "\n",
    "## Step 2: Experiments\n",
    "\n",
    "Below, we simply run optimizations with all algorithms in a loop and store the result \n",
    "in a dictionary. We limit the number of function evaluations to 8. Since some algorithms \n",
    "only support a maximum number of iterations as stopping criterion we also limit the \n",
    "number of iterations to 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for algo in [\"scipy_lbfgsb\", \"nlopt_lbfgsb\", \"fides\"]:\n",
    "    results[algo] = om.minimize(\n",
    "        fun=trid_scalar,\n",
    "        jac=trid_gradient,\n",
    "        params=np.arange(20),\n",
    "        algorithm=algo,\n",
    "        algo_options={\"stopping_maxfun\": 8, \"stopping_maxiter\": 8},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Comparison\n",
    "\n",
    "Next we plot the optimizer histories to find out which optimizer worked best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = om.criterion_plot(results, max_evaluations=8)\n",
    "fig.show(renderer=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All optimizers work pretty well here and since this is a very simple problem, any of them \n",
    "would probably find the optimum in a reasonable time. However, `nlopt_lbfgsb` is a bit \n",
    "better than the others, so we will select it for the next step. \n",
    "\n",
    "## Step 4: Optimization \n",
    "\n",
    "All that is left to do is to run the optimization until convergence with the best \n",
    "optimizer. To avoid duplicated calculations, we can already start from the previously \n",
    "best parameter vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_x = results[\"nlopt_lbfgsb\"].params\n",
    "results[\"nlopt_lbfgsb_complete\"] = om.minimize(\n",
    "    fun=trid_scalar,\n",
    "    jac=trid_gradient,\n",
    "    params=best_x,\n",
    "    algorithm=\"nlopt_lbfgsb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the result in a criterion plot we can see that the optimizer converges after \n",
    "a bit more than 30 function evaluations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = om.criterion_plot(results)\n",
    "fig.show(renderer=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(four-steps-variations)=\n",
    "\n",
    "## Variations\n",
    "\n",
    "The four steps described above work very well in most situations. However, sometimes \n",
    "it makes sense to deviate: \n",
    "\n",
    "- If you are unsure about some of the questions in step 1, select more algorithms for \n",
    "the experimentation phase and run more than 1 algorithm until convergence. \n",
    "- If it is very important to find a precise optimum, run more than 1 algorithm until \n",
    "convergence. \n",
    "- If you have a very fast objective function, simply run all candidate algorithms until \n",
    "convergence. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
