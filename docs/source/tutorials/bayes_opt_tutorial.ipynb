{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# `bayes_opt` Optimizer in optimagic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to use the `\"bayes_opt\"` optimizer in optimagic. To use it, you need to have `bayesian-optimization` package installed. You can install it with the following command:\n",
    "```bash\n",
    "pip install bayesian-optimization\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### When to use Bayesian Optimization:\n",
    "- Function evaluations are expensive (e.g., simulations, experiments)\n",
    "- The function is a black box(it cannot be expressed in closed form)\n",
    "- You have a limited budget of function evaluations\n",
    "- When gradients are unavailable or computationally expensive to obtain\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "### Gaussian Processes (GP)\n",
    "The GP serves as a probabilistic model of your objective function. It provides both a mean prediction and uncertainty estimates.\n",
    "### Acquisition Functions\n",
    "These functions use the GP's predictions to decide where to evaluate next.\n",
    "\n",
    "Common acquisition functions include:\n",
    "- **Upper Confidence Bound (UCB)**: Balances mean prediction with uncertainty\n",
    "- **Expected Improvement (EI)**: Expected improvement over the current best\n",
    "- **Probability of Improvement (POI)**: Probability of improving over the current best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import optimagic as om\n",
    "from bayes_opt import acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Basic Usage of the `bayes_opt` Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Let's start with a simple example using a sphere function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def sphere(params):\n",
    "    return params @ params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Note: bayes_opt requires bounds for all parameters\n",
    "bounds = om.Bounds(\n",
    "    lower=np.full(2, -10.0),\n",
    "    upper=np.full(2, 10.0)\n",
    ")\n",
    "bayesopt_res = om.minimize(\n",
    "    fun=sphere,\n",
    "    params=np.arange(2),\n",
    "    algorithm=\"bayes_opt\",\n",
    "    bounds=bounds,\n",
    "    algo_options={\"seed\": 1}\n",
    ")\n",
    "\n",
    "bayesopt_res.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Acquisition Functions in the `bayes_opt` Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "In Bayesian optimization, the **acquisition function** decides *where to sample next*.\n",
    "It controls the trade-off between **exploration** (search new areas) and **exploitation** (focus on good areas).\n",
    "\n",
    "optimagic lets you set the acquisition function in different ways:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### 1. Using No Acquisition Function (Default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Uses package defaults: UCB for unconstrained, EI for constrained\n",
    "acquisition_function = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### 2. Using a String (Built-in acquisition functions)\n",
    "\n",
    "You can pass any of the following strings to select a standard acquisition function:\n",
    "\n",
    "* `\"ucb\"` / `\"upper_confidence_bound\"` – Upper Confidence Bound\n",
    "* `\"ei\"` / `\"expected_improvement\"` – Expected Improvement\n",
    "* `\"poi\"` / `\"probability_of_improvement\"` – Probability of Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "acquisition_function_str = \"ucb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### 3. Using a Class (Auto-Instantiated)\n",
    "\n",
    "You can also pass the class directly, optimagic will create an instance for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from bayes_opt.acquisition import UpperConfidenceBound\n",
    "\n",
    "acquisition_function_class = UpperConfidenceBound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### 4. Using an Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from bayes_opt.acquisition import ExpectedImprovement\n",
    "\n",
    "acquisition_function_instance = ExpectedImprovement(\n",
    "    xi=0.1,\n",
    "    exploration_decay=0.95,\n",
    "    exploration_decay_delay=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Example Run with configured acquisition functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "res = om.minimize(\n",
    "    fun=sphere ,\n",
    "    params=np.arange(2),\n",
    "    algorithm=\"bayes_opt\",\n",
    "    bounds=om.Bounds(lower=np.full(2, -5.0), upper=np.full(2, 5.0)),\n",
    "    algo_options={\"seed\":1, \"acquisition_function\": acquisition_function_str,}\n",
    "        # acquisition_function can be any of:\n",
    "        #   acquisition_function_str        → e.g. \"ucb\", \"ei\", \"poi\"\n",
    "        #   acquisition_function_class      → e.g. UpperConfidenceBound\n",
    "        #   acquisition_function_instance   → e.g. ExpectedImprovement(xi=0.1)\n",
    "        #   None                            → defaults to \"ucb\"\n",
    "    )\n",
    "\n",
    "res.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Custom Acquisition Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "`bayesian-optimization` also allows us to write our own acquisition functions by subclassing its `AcquisitionFunction` class. This allows you to define exploration/exploitation strategies tailored to your specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Implementation Requirements\n",
    "\n",
    "When subclassing `AcquisitionFunction`, there are specific methods we must implement:\n",
    "\n",
    "1. **`base_acq(self, mean, std)` method (Required)**: This is the core method where you define the mathematical formula for your acquisition function. It takes the predicted mean and standard deviation from the Gaussian Process and returns the acquisition value(s).\n",
    "\n",
    "2. **`suggest` method (Optional but often needed)**: The base class provides a default implementation, but you may need to override it if you need to set up internal state (like `y_max` for EI/PI) before `base_acq` is called.\n",
    "\n",
    "3. **`get_acquisition_params` and `set_acquisition_params` methods (Optional but recommended)**: These are used for retrieving and setting the internal parameters of your acquisition function. Implementing them makes your acquisition function fully configurable and serializable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from bayes_opt.acquisition import AcquisitionFunction\n",
    "class CustomAcquisition(AcquisitionFunction):\n",
    "    \"\"\"\n",
    "    A simple custom acquisition function.\n",
    "\n",
    "    This acquisition returns half of the predicted mean.\n",
    "    It ignores the uncertainty (std), making it purely\n",
    "    exploitation-oriented.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def base_acq(self, mean, std):\n",
    "        return 0.5 * mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Using the Custom Acquisition Function\n",
    "\n",
    "Once you have defined your custom acquisition function, you can use it in optimagic by passing an instance or a class to the `acquisition_function` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "acquisition_function = CustomAcquisition()\n",
    "\n",
    "res = om.minimize(\n",
    "    fun=sphere ,\n",
    "    params=np.arange(2),\n",
    "    algorithm=\"bayes_opt\",\n",
    "    bounds=om.Bounds(lower=np.full(2, -5.0), upper=np.full(2, 5.0)),\n",
    "    algo_options={\"seed\":1, \"acquisition_function\": acquisition_function,}\n",
    "    )\n",
    "\n",
    "res.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Meta Acquisition Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "The `bayesian-optimization` package also provides meta acquisition functions that operate on other acquisition functions:\n",
    "\n",
    "1. **GPHedge**: Dynamically chooses the best acquisition function from a set of candidates based on their past performance.\n",
    "2. **ConstantLiar**: Used for parallelized optimization to discourage sampling near points that have already been suggested but not yet evaluated.\n",
    "\n",
    "Here's how to use GPHedge with multiple base acquisition functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### 1. **GPHedge**:\n",
    "Dynamically chooses the best acquisition function from a set of candidates based on their past performance.\n",
    "\n",
    "let’s define the **Branin function**, to use with Meta Acquisition functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def branin(params):\n",
    "    \"\"\"The Branin function - a classic optimization test function.\n",
    "\n",
    "    Has three global minima at approximately:\n",
    "    - (-π, 12.275)\n",
    "    - (π, 2.275)\n",
    "    - (9.42478, 2.475)\n",
    "\n",
    "    Global minimum value: 0.397887\n",
    "    \"\"\"\n",
    "    x1, x2 = params[0], params[1]\n",
    "\n",
    "    a = 1\n",
    "    b = 5.1 / (4 * np.pi**2)\n",
    "    c = 5 / np.pi\n",
    "    r = 6\n",
    "    s = 10\n",
    "    t = 1 / (8 * np.pi)\n",
    "\n",
    "    term1 = a * (x2 - b * x1**2 + c * x1 - r)**2\n",
    "    term2 = s * (1 - t) * np.cos(x1)\n",
    "    term3 = s\n",
    "\n",
    "    return term1 + term2 + term3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from bayes_opt.acquisition import GPHedge, UpperConfidenceBound, ExpectedImprovement\n",
    "\n",
    "# Create a list of base acquisition functions\n",
    "base_acquisitions = [\n",
    "    UpperConfidenceBound(kappa=2.576),\n",
    "    ExpectedImprovement(xi=0.01),\n",
    "    # Add more as needed\n",
    "]\n",
    "\n",
    "gphedge_acq = GPHedge(base_acquisitions)\n",
    "\n",
    "result = om.minimize(\n",
    "    fun=branin,\n",
    "    params=np.array([1.0, 1.0]),\n",
    "    algorithm=\"bayes_opt\",\n",
    "    bounds=bounds,\n",
    "    algo_options={\n",
    "        \"acquisition_function\": gphedge_acq,\n",
    "        \"seed\": 42\n",
    "    }\n",
    ")\n",
    "\n",
    "result.params, result.fun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### 2. ConstantLiar\n",
    "\n",
    "`ConstantLiar` is used for parallelized optimization. It discourages sampling near points that have already been suggested but not yet evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from bayes_opt.acquisition import ConstantLiar, UpperConfidenceBound\n",
    "\n",
    "base_acq = UpperConfidenceBound(kappa=2.576)\n",
    "\n",
    "constant_liar_acq = ConstantLiar(base_acquisition=base_acq, strategy=\"max\")\n",
    "\n",
    "# Use in optimization (Note: ConstantLiar is primarily for async optimization)\n",
    "result = om.minimize(\n",
    "    fun=sphere,\n",
    "    params=np.array([1.0, 1.0]),\n",
    "    algorithm=\"bayes_opt\",\n",
    "    bounds=bounds,\n",
    "    algo_options={\n",
    "        \"acquisition_function\": constant_liar_acq,\n",
    "        \"seed\": 42\n",
    "    }\n",
    ")\n",
    "\n",
    "result.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Exploration vs Exploitation Trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "When using Bayesian optimization, the acquisition function decides where to sample next. It balances exploration (try new areas) vs exploitation (refine known good areas).\n",
    "\n",
    "- **Exploration**: Sampling in regions with high uncertainty\n",
    "- **Exploitation**: Sampling in regions with high predicted values\n",
    "\n",
    "### Related Parameters\n",
    "\n",
    "- **kappa** (UCB): Higher values → more exploration\n",
    "- **xi** (EI/POI): Higher values → more exploration\n",
    "- **exploration_decay**: Gradually shift from exploration to exploitation\n",
    "- **exploration_decay_delay**: When to start the decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "  \"\"\"Function with multiple peaks\"\"\"\n",
    "  x = x[0]\n",
    "  return float(\n",
    "        np.exp(-(x - 2) ** 2) +\n",
    "        np.exp(-(x - 6) ** 2 / 10) +\n",
    "        1 / (x ** 2 + 1)\n",
    "    )\n",
    "x = np.linspace(-2, 10, 100)\n",
    "Y = [f([xi]) for xi in x]\n",
    "plt.plot(x, Y)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def plot_bayes_opt(result):\n",
    "    \"\"\"Plot optimization results\"\"\"\n",
    "    evaluated_points = np.array([p[0] for p in result.history.params])\n",
    "    function_values = np.array(result.history.fun)\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(x, Y, 'b-', label=\"Original function f(x)\")\n",
    "    plt.scatter(evaluated_points, function_values, c=\"red\", s=60, zorder=3, label=\"Evaluated points\")\n",
    "    plt.axvline(result.params[0], color=\"green\", linestyle=\"--\", label=\"Best param\")\n",
    "\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"f(x)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# strategy: exploitation (kappa=0.1) - focuses on known good areas\n",
    "acquisition_function = acquisition.UpperConfidenceBound(kappa=0.1)\n",
    "result = om.maximize(\n",
    "    fun=f,\n",
    "    params=np.array([0.]),\n",
    "    algorithm=\"bayes_opt\",\n",
    "    bounds=om.Bounds(lower=np.full(1, -2.0), upper=np.full(1, 10.0)),\n",
    "    algo_options={\n",
    "        \"acquisition_function\": acquisition_function,\n",
    "        \"seed\": 987234,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Notice: Points cluster around peaks, might also get stuck in local optimum\n",
    "plot_bayes_opt(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# strategy: exploration (kappa=10) - explores more broadly\n",
    "acquisition_function = acquisition.UpperConfidenceBound(kappa=10)\n",
    "result = om.maximize(\n",
    "    fun=f,\n",
    "    params=np.array([0.]),\n",
    "    algorithm=\"bayes_opt\",\n",
    "    bounds=om.Bounds(lower=np.full(1, -2.0), upper=np.full(1, 10.0)),\n",
    "    algo_options={\n",
    "        \"acquisition_function\": acquisition_function,\n",
    "        \"seed\": 987234,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Notice: Points are more spread out, better chance of finding global optimum\n",
    "plot_bayes_opt(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## Sequential Domain Reduction (SDR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "Sequential Domain Reduction (SDR) progressively narrows the search space around promising regions. This can significantly improve optimization, especially for high-dimensional problems.\n",
    "\n",
    "### SDR Parameters\n",
    "\n",
    "- `enable_sdr`: Enable/disable Sequential Domain Reduction\n",
    "- `sdr_gamma_osc`: Controls oscillation damping (default: 0.7)\n",
    "- `sdr_gamma_pan`: Controls panning behavior (default: 1.0)\n",
    "- `sdr_eta`: Zooming parameter for region shrinking (default: 0.9)\n",
    "- `sdr_minimum_window`:  Minimum window size (default: 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### SDR Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def ackley(x):\n",
    "    \"\"\"Global minimum: f(x*) = 0 at x* = (0, 0)\"\"\"\n",
    "    x0, x1 = x\n",
    "    arg1 = -0.2 * np.sqrt(0.5 * (x0 ** 2 + x1 ** 2))\n",
    "    arg2 = 0.5 * (np.cos(2 * np.pi * x0) + np.cos(2 * np.pi * x1))\n",
    "    return -20. * np.exp(arg1) - np.exp(arg2) + 20. + np.e\n",
    "\n",
    "start_params = np.array([2.0, 2.0])\n",
    "bounds = om.Bounds(\n",
    "    lower=np.array([-32.768, -32.768]),\n",
    "    upper=np.array([32.768, 32.768])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Standard Bayesian Optimization without SDR\n",
    "result_standard = om.minimize(\n",
    "    fun=ackley,\n",
    "    params=start_params,\n",
    "    algorithm=\"bayes_opt\",\n",
    "    bounds=bounds,\n",
    "    algo_options={\n",
    "        \"enable_sdr\": False,\n",
    "        \"n_iter\": 50,\n",
    "        \"init_points\": 2,\n",
    "        \"seed\": 1,\n",
    "        \"acquisition_function\": \"ucb\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Standard Bayesian Optimization:\")\n",
    "print(\"Best function value:\", result_standard.fun)\n",
    "print(\"Best parameters:\", result_standard.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Bayesian Optimization with SDR\n",
    "result_sdr = om.minimize(\n",
    "    fun=ackley,\n",
    "    params=start_params,\n",
    "    algorithm=\"bayes_opt\",\n",
    "    bounds=bounds,\n",
    "    algo_options={\n",
    "        \"enable_sdr\": True,\n",
    "        \"sdr_minimum_window\": 0.5,\n",
    "        \"sdr_gamma_osc\": 0.7,\n",
    "        \"sdr_gamma_pan\": 1.0,\n",
    "        \"sdr_eta\": 0.9,\n",
    "        \"n_iter\": 50,\n",
    "        \"init_points\": 2,\n",
    "        \"seed\": 1,\n",
    "        \"acquisition_function\": \"ucb\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Bayesian Optimization with SDR:\")\n",
    "print(\"Best function value:\", result_sdr.fun)\n",
    "print(\"Best parameters:\", result_sdr.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Compare convergence behavior\n",
    "results = {\n",
    "    \"Standard BO\": result_standard,\n",
    "    \"BO with SDR\": result_sdr\n",
    "}\n",
    "\n",
    "# SDR typically converges faster than standard BO\n",
    "fig = om.criterion_plot(results)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "## Gaussian Process Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "`\"bayesian-optimization\"` uses a Gaussian Process (GP) as the surrogate model. Its behavior can be tuned with these options via algo_options:\n",
    "\n",
    "\n",
    "* **alpha**: noise level in function evaluations\n",
    "\n",
    "  * lower values (e.g.,`1e-6`): assumes nearly precise function evaluations\n",
    "  * higher values (e.g., `1e-2`): assumes noisy evaluations\n",
    "\n",
    "* **n\\_restarts**: Number of times to restart the optimization.\n",
    "\n",
    "* **seed** → ensures reproducible results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "algo_options = {\n",
    "    \"alpha\": 1e-3,\n",
    "    \"n_restarts\": 5,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "result_configured = om.minimize(\n",
    "    fun=sphere,\n",
    "    params=np.array([3.0, 3.0]),\n",
    "    algorithm=\"bayes_opt\",\n",
    "    bounds=om.Bounds(lower=np.full(2, -5.0), upper=np.full(2, 5.0)),\n",
    "    algo_options=algo_options\n",
    ")\n",
    "\n",
    "print(\"Configured GP results:\")\n",
    "print(f\"  Best value: {result_configured.fun}\")\n",
    "print(f\"  Function evaluations: {result_configured.n_fun_evals}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Bayesian optimization is a powerful tool for optimizing expensive black-box functions. Key takeaways:\n",
    "\n",
    "1. **Choose the right acquisition function** based on your exploration/exploitation needs\n",
    "2. **Tune acquisition parameters** like kappa (UCB) or xi (EI) to control the trade-off\n",
    "3. **Use SDR** for high-dimensional problems to focus the search\n",
    "4. **Configure the GP properly** with appropriate noise levels and restarts\n",
    "\n",
    "For more detailed information, check out the [bayesian-optimization documentation](https://bayesian-optimization.github.io/BayesianOptimization/3.1.0/index.html#)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
