{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical optimization\n",
    "\n",
    "Using simple examples, this tutorial shows how to do an optimization with optimagic. More details on the topics covered here can be found in the [how to guides](../how_to/index.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import optimagic as om\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic usage of `minimize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sphere(params):\n",
    "    return params @ params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = om.minimize(\n",
    "    fun=sphere,\n",
    "    params=np.arange(5),\n",
    "    algorithm=\"scipy_lbfgsb\",\n",
    ")\n",
    "\n",
    "res.params.round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `params` do not have to be vectors\n",
    "\n",
    "In optimagic, params can by arbitrary [pytrees](https://jax.readthedocs.io/en/latest/pytrees.html). Examples are (nested) dictionaries of numbers, arrays and pandas objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_sphere(params):\n",
    "    return params[\"a\"] ** 2 + params[\"b\"] ** 2 + (params[\"c\"] ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = om.minimize(\n",
    "    fun=dict_sphere,\n",
    "    params={\"a\": 0, \"b\": 1, \"c\": pd.Series([2, 3, 4])},\n",
    "    algorithm=\"scipy_powell\",\n",
    ")\n",
    "\n",
    "res.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The result contains all you need to know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = om.minimize(\n",
    "    fun=dict_sphere,\n",
    "    params={\"a\": 0, \"b\": 1, \"c\": pd.Series([2, 3, 4])},\n",
    "    algorithm=\"scipy_neldermead\",\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can visualize the convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = om.criterion_plot(res, max_evaluations=300)\n",
    "fig.show(renderer=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = om.params_plot(\n",
    "    res,\n",
    "    max_evaluations=300,\n",
    "    # optionally select a subset of parameters to plot\n",
    "    selector=lambda params: params[\"c\"],\n",
    ")\n",
    "fig.show(renderer=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are many optimizers\n",
    "\n",
    "If you install some optional dependencies, you can choose from a large (and growing) set of optimization algorithms -- all with the same interface!\n",
    "\n",
    "For example, we wrap optimizers from `scipy.optimize`, `nlopt`, `cyipopt`, `pygmo`, `fides`, `tao` and others. \n",
    "\n",
    "We also have some optimizers that are not part of other packages. Examples are a `parallel Nelder-Mead` algorithm, The `BHHH` algorithm and a `parallel Pounders` algorithm.\n",
    "\n",
    "See the full list [here](../how_to_guides/optimization/how_to_specify_algorithm_and_algo_options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can add bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = om.Bounds(lower=np.arange(5) - 2, upper=np.array([10, 10, 10, np.inf, np.inf]))\n",
    "\n",
    "res = om.minimize(\n",
    "    fun=sphere,\n",
    "    params=np.arange(5),\n",
    "    algorithm=\"scipy_lbfgsb\",\n",
    "    bounds=bounds,\n",
    ")\n",
    "\n",
    "res.params.round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can fix parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = om.minimize(\n",
    "    fun=sphere,\n",
    "    params=np.arange(5),\n",
    "    algorithm=\"scipy_lbfgsb\",\n",
    "    constraints=[{\"loc\": [1, 3], \"type\": \"fixed\"}],\n",
    ")\n",
    "\n",
    "res.params.round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or impose other constraints\n",
    "\n",
    "As an example, let's impose the constraint that the first three parameters are valid probabilities, i.e. they are between zero and one and sum to one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = om.minimize(\n",
    "    fun=sphere,\n",
    "    params=np.array([0.1, 0.5, 0.4, 4, 5]),\n",
    "    algorithm=\"scipy_lbfgsb\",\n",
    "    constraints=[{\"loc\": [0, 1, 2], \"type\": \"probability\"}],\n",
    ")\n",
    "\n",
    "res.params.round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a full overview of the constraints we support and the corresponding syntaxes, check out [the documentation](../how_to/how_to_constraints.md).\n",
    "\n",
    "Note that `\"scipy_lbfgsb\"` is not a constrained optimizer. If you want to know how we achieve this, check out [the explanations](../explanation/implementation_of_constraints.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There is also maximize\n",
    "\n",
    "If you ever forgot to switch back the sign of your criterion function after doing a maximization with `scipy.optimize.minimize`, there is good news:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upside_down_sphere(params):\n",
    "    return -params @ params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = om.maximize(\n",
    "    fun=upside_down_sphere,\n",
    "    params=np.arange(5),\n",
    "    algorithm=\"scipy_bfgs\",\n",
    ")\n",
    "\n",
    "res.params.round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimagic got your back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can provide closed form derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sphere_gradient(params):\n",
    "    return 2 * params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = om.minimize(\n",
    "    fun=sphere,\n",
    "    params=np.arange(5),\n",
    "    algorithm=\"scipy_lbfgsb\",\n",
    "    jac=sphere_gradient,\n",
    ")\n",
    "res.params.round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or use parallelized numerical derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = om.minimize(\n",
    "    fun=sphere,\n",
    "    params=np.arange(5),\n",
    "    algorithm=\"scipy_lbfgsb\",\n",
    "    numdiff_options=om.NumdiffOptions(n_cores=6),\n",
    ")\n",
    "\n",
    "res.params.round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn local optimizers global with multistart\n",
    "\n",
    "Multistart optimization requires finite soft bounds on all parameters. Those bounds will\n",
    "be used for sampling but not enforced during optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = om.Bounds(soft_lower=np.full(10, -5), soft_upper=np.full(10, 15))\n",
    "\n",
    "res = om.minimize(\n",
    "    fun=sphere,\n",
    "    params=np.arange(10),\n",
    "    algorithm=\"scipy_neldermead\",\n",
    "    bounds=bounds,\n",
    "    multistart=om.MultistartOptions(convergence_max_discoveries=5),\n",
    ")\n",
    "res.params.round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And plot the criterion history of all local optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = om.criterion_plot(res)\n",
    "fig.show(renderer=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploit the structure of your optimization problem\n",
    "\n",
    "Many estimation problems have a least-squares structure. If so, specialized optimizers that exploit this structure can be much faster than standard optimizers. The `sphere` function from above is the simplest possible least-squarse problem you could imagine: the least-squares residuals are just the params. \n",
    "\n",
    "To use least-squares optimizers in optimagic, you need to declare mark your function with \n",
    "a decorator and return the least-squares residuals instead of the aggregated function value. \n",
    "\n",
    "More details can be found [here](../how_to/how_to_criterion_function.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@om.mark.least_squares\n",
    "def ls_sphere(params):\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = om.minimize(\n",
    "    fun=ls_sphere,\n",
    "    params=np.arange(5),\n",
    "    algorithm=\"pounders\",\n",
    ")\n",
    "res.params.round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, any least-squares problem can also be solved with a standard optimizer. \n",
    "\n",
    "There are also specialized optimizers for likelihood functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using and reading persistent logging\n",
    "\n",
    "For long-running and difficult optimizations, it can be worthwhile to store the progress in a persistent log file. You can do this by providing a path to the `logging` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = om.minimize(\n",
    "    fun=sphere,\n",
    "    params=np.arange(5),\n",
    "    algorithm=\"scipy_lbfgsb\",\n",
    "    logging=\"my_log.db\",\n",
    "    log_options={\"if_database_exists\": \"replace\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read the entries in the log file (while the optimization is still running or after it has finished) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = om.OptimizeLogReader(\"my_log.db\")\n",
    "reader.read_history().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on what you can do with the log file and LogReader object, check out [the logging tutorial](../how_to/how_to_logging.ipynb)\n",
    "\n",
    "The persistent log file is always instantly synchronized when the optimizer tries a new parameter vector. This is very handy if an optimization has to be aborted and you want to extract the current status. It can be displayed in `criterion_plot` and `params_plot`, even while the optimization is running. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize your optimizer\n",
    "\n",
    "Most algorithms have a few optional arguments. Examples are convergence criteria or tuning parameters. You can find an overview of supported arguments [here](../how_to/how_to_specify_algorithm_and_algo_options.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_options = {\n",
    "    \"convergence.ftol_rel\": 1e-9,\n",
    "    \"stopping.maxiter\": 100_000,\n",
    "}\n",
    "\n",
    "res = om.minimize(\n",
    "    fun=sphere,\n",
    "    params=np.arange(5),\n",
    "    algorithm=\"scipy_lbfgsb\",\n",
    "    algo_options=algo_options,\n",
    ")\n",
    "res.params.round(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
